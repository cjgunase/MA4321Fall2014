---
title: "EX6"
author: "Chathura J Gunasekara"
date: "Thursday, October 16, 2014"
output: word_document
---
1.
a. Load the data

```{r,echo=FALSE}
rm(list = ls())
library(caret)
data(tecator)
tecator <-as.data.frame(cbind(endpoints[,2],absorp))
```

b. Use PCA to determine the effective dimension of the data which is number of PCA Componenents. There are two PCA compnenets which is the effective dimention.

```{r, echo=FALSE}
print ("Using Scree Plot to check number of effective compents",quote = FALSE)

screeplot(princomp(absorp),type="lines",main="Scree plot to check Principal Components")

trans <- preProcess(as.matrix(absorp),method=c("center","scale", "pca"))
PC <- predict(trans, absorp)
head(PC)
```
c.Using the created 2 PCs a traing and testing set is created.
```{r,echo=FALSE}
newdata <-as.data.frame(cbind(endpoints[,2],PC))
head(newdata)
smp_size <- floor(0.75 * nrow(newdata))
train_ind <- sample(seq_len(nrow(newdata)), size = smp_size)
train <- newdata[train_ind, ]
test <- newdata[-train_ind, ]

```
i. Ordinary Linear Regression
```{r,echo=FALSE}
PCAlmfit <-lm(V1~.,data=newdata)
summary(PCAlmfit)
print("Testing data",quote=FALSE)
testlmfit <- data.frame(obs = test$V1, pred = predict(PCAlmfit,test))
defaultSummary(testlmfit)
```
ii. Robust Linear Regresssion
```{r,echo=FALSE}
library(MASS)
PCArlmfit <-rlm(V1~.,data=newdata)
summary(PCArlmfit)
print("Testing data",quote=FALSE)
testrlmfit <- data.frame(obs = test$V1, pred = predict(PCArlmfit,test))
defaultSummary(testrlmfit)
```
iii. Partial Least Squares
```{r,echo=FALSE}
library(pls)
PCArplsrfit <-plsr(V1~.,data=newdata)
summary(PCArplsrfit)
print("Testing data",quote=FALSE)
predict(PCArplsrfit, test[1:5,],ncomp=1:2)
plsTune <- train(PC, endpoints[,2],method = "pls",tuneLength = 20,preProc = c("center", "scale"))
plsTune
```
iv.ridge regresssion
```{r,echo=FALSE}
library(elasticnet)
ridgeModel <- enet(x = as.matrix(train[,2:3]), y = train$V1,lambda = 0.001)
ridgePred <- predict(ridgeModel, newx = as.matrix(test[,2:3]),s = 1, mode = "fraction",type = "fit")
head(ridgePred$fit)


library(caret)
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))
set.seed(100)
ridgeRegFit <- train(train[,2:3], train$V1,method = "ridge",tuneGrid = ridgeGrid,preProc = c("center", "scale"))
ridgeRegFit




```
v.lasso model
```{r,echo=FALSE}
enetModel <- enet(x = as.matrix(train[,2:3]), y = train$V1,lambda = 0.01, normalize = TRUE)

enetPred <- predict(enetModel, newx = as.matrix(test[,2:3]),s = .1, mode = "fraction",type = "fit")
names(enetPred)
head(enetPred$fit)

enetCoef <- predict(enetModel, newx = as.matrix(test[,2:3]),s = .1, mode = "fraction",type = "coefficients")
tail(enetCoef$coefficients)
```
(d)For this data set non of the models are siginificantly better or worse than others.A robust linear model with PCA should work because its interpretablity. 

(e) Robust Linear model will be used because it is easier to implement and give higer R-squred values for the fit and lower RMSE for testing data.

2.
a.Loading data
```{r,echo=FALSE}
rm(list = ls())
library(AppliedPredictiveModeling)
data(permeability)
```
b. Remove the near zero variance predictors.
```{r,echo=FALSE}
library(caret)
newfingerprints <- fingerprints[,-nearZeroVar(fingerprints)]
ncol(newfingerprints)
newfingerprints<-as.data.frame(newfingerprints)
print("New Dimension of the fingerprint is:",quote = FALSE,max.levels = NULL)
dim(newfingerprints)
```
c.Split the data and train a PLS model
```{r,echo=FALSE}
dataset <-as.data.frame(cbind(permeability,newfingerprints))
smp_size <- floor(0.75 * nrow(dataset))
set.seed(123)
train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)
train <- dataset[train_ind, ]
test <- dataset[-train_ind, ]

library(pls)
plsTune <- train(train[,-1], train[,1],method = "pls",tuneLength = 40,preProc = c("center", "scale"))
plsTune
```
Number of Componets is 6, Corresponding resampled R-squared = 0.419 
d.
```{r,echo=FALSE}
predTest <- predict(plsFit, test, ncomp=6)
predTest <-as.matrix(predTest)
plsSum <- data.frame(obs = test$permeability, pred = predTest[,1])
defaultSummary(plsSum)
```
RMSE   Rsquared 
13.7364443  0.4160687 
e.

```{r,echo=FALSE}
#lm
ctrl <- trainControl(method = "cv", number = 10)
lm <- train(train[,-1], train[,1],method = "lm",preProcess = "pca",trControl = ctrl)
summary(lm)
print(" Simple Linear Reg with PCA Testing data",quote=FALSE)
testlmfit <- data.frame(obs = test[,1], pred = predict(lm,test[,-1]))
defaultSummary(testlmfit)

#rlm
rlmPCA <- train(train[,-1], train[,1],method = "rlm",preProcess = "pca",trControl = ctrl)
rlmPCA
print(" Robust Linear Reg with PCA Testing data",quote=FALSE)
testrlmfit <- data.frame(obs = test[,1], pred = predict(rlmPCA,test[,-1]))
defaultSummary(testrlmfit)

#enet
library(elasticnet)
ridgeModel <- enet(x=as.matrix(train[,-1]),y=train[,1],lambda=0.1)
ridgepredTrain<-predict(ridgeModel,newx=as.matrix(train[,-1]),s=1,mode="fraction")
ridgepredTrain <- as.data.frame(ridgepredTrain)

enetTrain <- data.frame(obs = train[,1], pred = ridgepredTrain[,4])
print("training set")
defaultSummary(enetTrain)

ridgepredTest<-predict(ridgeModel,newx=as.matrix(test[,-1]),s=1,mode="fraction")
ridgepredTest <- as.data.frame(ridgepredTest)

enetTest <- data.frame(obs = test[,1], pred = ridgepredTest[,4])
print("testing set")
defaultSummary(enetTest)

#ridge
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))
ridgeRegFit <- train(train[,-1], train[,1],
method = "ridge",
 ## Fir the model over many penalty values
 tuneGrid = ridgeGrid,
 trControl = ctrl,
 ## put the predictors on the same scale
 preProc = c("center", "scale"))
ridgeRegFit


testridge <- data.frame(obs = test[,1], pred = predict(ridgeRegFit,test[,-1]))
defaultSummary(testridge)

#lasso
library(lars)
model.lasso <- lars(as.matrix(train[,-1]), train[,1], type="lasso")
predTrain <-predict(model.lasso,train[,-1])
str(as.data.frame(predTrain$fit))

trainlasso <- data.frame(obs = train[,1], pred = predTrain$fit)
print("train")
defaultSummary(trainlasso)
print("test")
predTrain <-predict(model.lasso,test[,-1])
testlasso <- data.frame(obs = test[,1], pred = predTrain$fit)
defaultSummary(testridge)

#http://www.stat.wisc.edu/~gvludwig/fall_2012/handout15.R

```

f.
No, All these models are not extreamly good at predicting. RMSE is high and R-squred is low in all of these linear models.


3.

a.Load Data
```{r,echo=FALSE}
rm(list = ls())
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
y<-ChemicalManufacturingProcess[,1]
x<-ChemicalManufacturingProcess[,-1]

```

b.Impute missing values
```{r,echo=FALSE}
source("http://bioconductor.org/biocLite.R")
biocLite("impute", suppressUpdates=TRUE)
library(impute)
print("used KNN with k=5 to impute the missing data")
x.imputed <- impute.knn(as.matrix(x), k=5)
dataset <-cbind(y,x.imputed$data)

```
c.Split the data in to training and testing set. Train a PLS model
```{r,echo=FALSE}
smp_size <- floor(0.75 * nrow(dataset))
train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)
train <- as.data.frame(dataset[train_ind, ])
test <- as.data.frame(dataset[-train_ind, ])

print("Random sampling method to split the data in to 75% for the training and 25% for testing.")
print("Used PLS regression with data centerd and scaled")


library(pls)
ctrl <- trainControl(method = "cv", number = 10)
plsTune <- train(train[,-1], train[,1],
 method = "pls",
 ## The default tuning grid evaluates
 ## components 1... tuneLength
 tuneLength = 20,
 trControl = ctrl,
 preProc = c("center", "scale"))
plsTune

plsFit <-plsr(y~.,data=train,3,validation="CV")
summary(plsFit)
plot(RMSEP(plsFit),legendpos="topright")

```

d.
```{r,echo=FALSE}
ypred<-predict(plsFit, ncomp = 3, newdata = test[,-1])
ypred <- as.data.frame(ypred)

plsTestValues1 <- data.frame(obs =test[,1], pred = ypred[,1])
plsTestValues1
defaultSummary(plsTestValues1)
```
e.

```{r,echo=FALSE}
library(caret)
plsImp <- varImp(plsFit,useModel=TRUE,scale=FALSE)
plsImp

```
ManufacturingProcess variables dominate the list with higher weighted sums.


f.
```{r,echo=FALSE}
dataset <- as.data.frame(dataset)
plot(dataset$ManufacturingProcess36,dataset$y,ylab="yeild",xlab="ManufacturingProcess36")
plot(dataset$ManufacturingProcess03,dataset$y,ylab="yeild",xlab="ManufacturingProcess03")

```
ManufacturingProcess36 has a high overall weight and when we plot yeild and ManufacturingProcess36 has a high correlation can be seen. But for ManufacturingProcess03 , it is low. So information about ManufacturingProcess36 is helpfull in improving the yeild.


