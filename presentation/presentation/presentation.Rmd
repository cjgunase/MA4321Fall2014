---
title: "Character Classificatin UCI ML"
output: word_document
---

rm(list=ls())
Loading and Preprocessing : removeing near zero variance predictors and high correlated predictors
```{r,echo=TRUE}
Data <- read.csv("./letter-recognition.data",sep=",",header=FALSE)
#Class Frequency Distribution
y <- Data[,1]
barplot(table(y),xlab="Class",col="purple",main="ClassFrequcy Distribution")
library(corrplot)
library(caret)
x <- Data[,-1]
#Correlations between predictors.
correlations <-cor(x)
corrplot(correlations, order = "hclust")
x<-x[,-findCorrelation(correlations,cutoff=0.75)]
nearZeroVar(x)
smp_size <- floor(0.75 * nrow(x))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(x)), size = smp_size)

trainX <- x[train_ind, ]
trainY <- y[train_ind]
testX <-  x[-train_ind, ]
testY<-y[-train_ind]
```
Linear Models

```{r,echo=TRUE}

ldaModel<- train(x = trainX,y = trainY,method = "lda2",metric = "Kappa",preProc = c("center", "scale"))
ldaModel
ldaPred <- predict(ldaModel,testX)
confusionMatrix(ldaPred,testY)

#pls
plsdaModel <- train(x = trainX,y = trainY,method = "pls",tuneGrid = expand.grid(.ncomp = 1:2),preProc = c("center","scale"),metric = "Kappa")
plsdaModel
plsdaPred <- predict(plsdaModel,testX)
confusionMatrix(plsdaPred,testY)

#penalized
glmnGrid <- expand.grid(.alpha = c(0, .1),.lambda=seq(0.1,0.2,length=10))
glmnTuned <- train(trainX,y=trainY,method="glmnet",tuneGrid=glmnGrid)
confusionMatrix(predict(glmnTuned,testX),testY)

#NSC
nscGrid <- data.frame(.threshold = 0:25)
set.seed(476)
nscTuned <- train(x = trainX,
                   y = trainY,method = "pam",
                   tuneGrid = nscGrid)
nscTuned
confusionMatrix(predict(nscTuned,testX),testY)
varImp(nscTuned,scale=FALSE)

```

Non - Linear models
KNN
```{r,echo=TRUE}

knnfit<- train(x = trainX,y = trainY,method = "knn",metric = "Kappa",preProc = c("center", "scale"),tuneGrid = data.frame(.k = c(3,5,7,9,11)))
library(class)
myknn<-knn(trainX,testX,as.factor(trainY), k = 3, l = 0, prob = FALSE, use.all = TRUE)
confusionMatrix(myknn,testY)
```

Improved KNN.
For the problem of nearest neighbor classification, a simpler approach called "leave-out-one" cross-validation can be used, and this is provided by the knn.cv function. Using this technique, the observation itself is ignored when looking for its neighbors.

```{r,echo=TRUE}
myknn<-knn.cv(x, y, k = 3, l = 0, prob = FALSE, use.all = TRUE)
confusionMatrix(myknn, y) 


```
SVM
```{r,echo=TRUE}
library(e1071)
library(kernlab)
library(klaR)
sigmaRange <- sigest(as.matrix(trainX))
svmRGrid <- expand.grid(.sigma = sigmaRange[1],.C = 2^(seq(-4, 4)))

svmRModel <- train(trainX, trainY,method = "svmRadial",metric = "Kappa",tuneGrid = svmRGrid,fit = FALSE,preProc = c("center", "scale"))
svmRModel
svmPred<-predict(svmRModel,testX)
confusionMatrix(svmPred,testY)
```
Nnet
```{r,echo=TRUE}
nnetGrid <-expand.grid(.size=1:10,.decay=c(0,0.1,1,2))
maxSize <- max(nnetGrid$.size)
numWts <- 1*(maxSize * (length(trainX) + 1) + maxSize + 1)
set.seed(476)
nnetModel <- train(x=trainX,y=trainY,method="nnet",metric="Kappa",tuneGrid=nnetGrid,preProc=c("center", "scale"),trace=FALSE,maxit=200,MaxNWts=numWts)
nnetModel
confusionMatrix(predict(nnetModel,testX),testY)

```
MDA
```{r,echo=TRUE}
library(mda)
mdaModel <- train(trainX,y = trainY,method = "mda",metric = "Kappa",tuneGrid = expand.grid(.subclasses=1:5))
mdaModel
mdaPred <-predict(mdaModel,testX)
confusionMatrix(mdaPred,testY)
plsImp <- varImp(mdaModel,scale=FALSE)
plsImp
```
```{r,echo=TRUE}

```
FDA
```{r,echo=TRUE}
fdaModel <- train(trainX,y = trainY,method = "fda",metric = "Kappa")
fdaModel
fdaPred <-predict(fdaModel,testX)
confusionMatrix(fdaPred,testY)
plsImp <- varImp(fdaModel,scale=FALSE)
plsImp
```
NB
```{r,echo=TRUE}
library(e1071)
classifier<-naiveBayes(trainX, trainY) 
confusionMatrix(predict(classifier, testX),testY)
```